{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb5979b",
   "metadata": {},
   "source": [
    "# Introduction to autonomous vehicles - Task 1 Perception\n",
    "\n",
    "## Task Description\n",
    "\n",
    "Select a Dataset of road traffic containing annotated objects. \n",
    "Please select a method and use this dataset to train a model to detect and classify **Pedestrians, Cyclists and Vehicles** on a Video Clip. \n",
    "The video clip could be from the dataset or from other sources.\n",
    "\n",
    "**Available Datasets:**\n",
    "\n",
    "- [KITTI Dataset](https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark)\n",
    "- [COCO Dataset](https://cocodataset.org/)\n",
    "- [Waymo Dataset](https://console.cloud.google.com/storage/browser/waymo_open_dataset_v_1_2_0_individual_files)\n",
    "  - **Note:** You might have to first register [here](https://waymo.com/open/) to get access to the dataset.\n",
    "\n",
    "**Rules:**\n",
    "\n",
    "- Use whatever framework you prefer (Pytroch, Tensorflow, ultralytics, etc.)\n",
    "- Recommended to use any version of YOLO\n",
    "- Use the code from the GitHub repository of the previous mentioned or other published methods (e.g. https://github.com/ultralytics/yolov3)\n",
    "- Use pretrained weights\n",
    "  - **Note:** You can use the pretrained weights, but you have to train and adapt them to your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ad3745",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "You should set up the environment accordingly. For now you can use the below code to install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install ipykernel\n",
    "!pip install jupyterlab\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install seaborn\n",
    "!pip install ultralytics\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5748d86",
   "metadata": {},
   "source": [
    "**Wandb settings:**\n",
    "\n",
    "Add a file named `wandb-api-key.txt` with your wandb API key in it so that we can login to wandb.\n",
    "\n",
    "```bash\n",
    "echo \"your_wandb_api_key\" > wandb-api-key.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd8d247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/lukas-kurz/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkurzlukas\u001b[0m (\u001b[33mkurzlukas-johannes-kepler-universit-t-linz\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Read wandb API key from file\n",
    "with open('../wandb-api-key.txt', 'r') as file:\n",
    "    wandb.login(key=file.read().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d31ff2",
   "metadata": {},
   "source": [
    "**Ultralytics settings:**\n",
    "\n",
    "Ultralytics might have some leftover settings from another run or from the previous task. You can use the following code to reset the settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "604edfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'settings_version': '0.0.6',\n",
       " 'datasets_dir': '/home/lukas-kurz/Desktop/autonomous_driving/intro_to_autonomous_vehicles_2025/datasets',\n",
       " 'weights_dir': '/home/lukas-kurz/Desktop/autonomous_driving/intro_to_autonomous_vehicles_2025/task1/weights',\n",
       " 'runs_dir': '/home/lukas-kurz/Desktop/autonomous_driving/intro_to_autonomous_vehicles_2025/task1/runs',\n",
       " 'uuid': 'f8376e8d76181ab673a86b3f562cfd88bbe4c72eeaa8b93b13c3c8c81b770686',\n",
       " 'sync': True,\n",
       " 'api_key': '',\n",
       " 'openai_api_key': '',\n",
       " 'clearml': True,\n",
       " 'comet': True,\n",
       " 'dvc': True,\n",
       " 'hub': True,\n",
       " 'mlflow': True,\n",
       " 'neptune': True,\n",
       " 'raytune': True,\n",
       " 'tensorboard': True,\n",
       " 'wandb': True,\n",
       " 'vscode_msg': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import settings\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Get current working directory from system\n",
    "working_dir = os.getcwd()\n",
    "settings['datasets_dir'] = Path(working_dir + '/../datasets').resolve().__str__()\n",
    "settings['weights_dir'] = Path(working_dir + '/weights').resolve().__str__()\n",
    "settings['runs_dir'] = Path(working_dir + '/runs').resolve().__str__()\n",
    "settings['wandb'] = True\n",
    "\n",
    "settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725fe953",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7315e62",
   "metadata": {},
   "source": [
    "For our first project we chose the provided **COCO Dataset**\n",
    "\n",
    "It is an open dataset for region segmentation, that is hosted as a challenge. As such, **the test set annotations are not available**.\n",
    "\n",
    "The COCO dataset has 80 object categories, including common objects like cars, bicycles, and animals, as well as more specific categories such as umbrellas, handbags, and sports equipment. For our task we will not need all of these, but luckily with the implementation of ultralytics we can select a subset of these to train on.\n",
    "\n",
    "Concretely we will group them as:\n",
    "### Pedestrians\n",
    "* Person\n",
    "* Dog\n",
    "* Cow\n",
    "* Horse\n",
    "* Cat\n",
    "\n",
    "### Vehicle\n",
    "* Motorcycle\n",
    "* Bus\n",
    "* Car\n",
    "* Truck\n",
    "* Train\n",
    "\n",
    "### Cyclist\n",
    "* Bicycle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750db610",
   "metadata": {},
   "source": [
    "TODO: visualize some of the stats of the dataset, as well as some example images with annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cbd62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d895f1f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5662fb55",
   "metadata": {},
   "source": [
    "For a first attempt, we can use ultralytics' implementation of a training without any augmentations, which we can add later to check and compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c196c",
   "metadata": {},
   "source": [
    "TODO: actually run and train the model with parameters setting like in: https://docs.ultralytics.com/modes/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95a82cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.86 üöÄ Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11143MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=coco128.yaml, epochs=2, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train7, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/home/lukas-kurz/Desktop/autonomous_driving/intro_to_autonomous_vehicles_2025/task1/runs/detect/train7\n",
      "\n",
      "Dataset 'coco128.yaml' images not found ‚ö†Ô∏è, missing path '/home/lukas-kurz/Desktop/autonomous_driving/intro_to_autonomous_vehicles_2025/datasets/coco128/images/train2017'\n",
      "Downloading https://ultralytics.com/assets/coco128.zip to '/home/lukas-kurz/Desktop/autonomous_driving/intro_to_autonomous_vehicles_2025/datasets/coco128.zip'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.66M/6.66M [00:00<00:00, 73.9MB/s]\n",
      "Unzipping /home/lukas-kurz/Desktop/autonomous_driving/intro_to_autonomous_vehicles_2025/datasets/coco128.zip to /home/lukas-kurz/Desktop/autonomous_driving/intro_to_autonomous_vehicles_2025/datasets/coco128...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 263/263 [00:00<00:00, 5734.12file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset download success ‚úÖ (1.1s), saved to \u001b[1m/home/lukas-kurz/Desktop/autonomous_driving/intro_to_autonomous_vehicles_2025/datasets\u001b[0m\n",
      "\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    464912  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "YOLO11n summary: 181 layers, 2,624,080 parameters, 2,624,064 gradients, 6.6 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/lukas-kurz/Desktop/autonomous_driving/intro_to_autonomous_vehicles_2025/task1/wandb/run-20250310_140109-3db6vcad</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kurzlukas-johannes-kepler-universit-t-linz/Ultralytics/runs/3db6vcad' target=\"_blank\">train7</a></strong> to <a href='https://wandb.ai/kurzlukas-johannes-kepler-universit-t-linz/Ultralytics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kurzlukas-johannes-kepler-universit-t-linz/Ultralytics' target=\"_blank\">https://wandb.ai/kurzlukas-johannes-kepler-universit-t-linz/Ultralytics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kurzlukas-johannes-kepler-universit-t-linz/Ultralytics/runs/3db6vcad' target=\"_blank\">https://wandb.ai/kurzlukas-johannes-kepler-universit-t-linz/Ultralytics/runs/3db6vcad</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/lukas-kurz/Desktop/autonomous_driving/intro_to_autonomous_vehicles_2025/datasets/coco128/labels/train2017... 126 images, 2 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 2508.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/lukas-kurz/Desktop/autonomous_driving/intro_to_autonomous_vehicles_2025/datasets/coco128/labels/train2017.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/lukas-kurz/Desktop/autonomous_driving/intro_to_autonomous_vehicles_2025/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/lukas-kurz/Desktop/autonomous_driving/intro_to_autonomous_vehicles_2025/task1/runs/detect/train7/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/lukas-kurz/Desktop/autonomous_driving/intro_to_autonomous_vehicles_2025/task1/runs/detect/train7\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2      2.63G      1.219      1.587      1.271        217        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03<00:00,  2.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        128        929      0.681      0.598      0.682      0.514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/2      2.66G      1.203      1.379      1.232        218        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:02<00:00,  3.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        128        929      0.696      0.594      0.685      0.519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2 epochs completed in 0.003 hours.\n",
      "Optimizer stripped from /home/lukas-kurz/Desktop/autonomous_driving/intro_to_autonomous_vehicles_2025/task1/runs/detect/train7/weights/last.pt, 5.5MB\n",
      "Optimizer stripped from /home/lukas-kurz/Desktop/autonomous_driving/intro_to_autonomous_vehicles_2025/task1/runs/detect/train7/weights/best.pt, 5.5MB\n",
      "\n",
      "Validating /home/lukas-kurz/Desktop/autonomous_driving/intro_to_autonomous_vehicles_2025/task1/runs/detect/train7/weights/best.pt...\n",
      "Ultralytics 8.3.86 üöÄ Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11143MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        128        929      0.709      0.592      0.685      0.519\n",
      "                person         61        254      0.819      0.677      0.782      0.544\n",
      "               bicycle          3          6       0.47      0.167      0.394      0.252\n",
      "                   car         12         46      0.733      0.196      0.248      0.169\n",
      "            motorcycle          4          5      0.723          1      0.995      0.789\n",
      "              airplane          5          6      0.838      0.833      0.955      0.854\n",
      "                   bus          5          7      0.779      0.714      0.727      0.659\n",
      "                 train          3          3      0.735      0.939       0.83      0.714\n",
      "                 truck          5         12      0.538       0.25      0.391      0.242\n",
      "                  boat          2          6       0.89        0.5      0.637      0.398\n",
      "         traffic light          4         14      0.513      0.143      0.242      0.166\n",
      "             stop sign          2          2      0.814          1      0.995      0.798\n",
      "                 bench          5          9      0.937      0.444      0.657      0.349\n",
      "                  bird          2         16      0.911          1      0.995      0.633\n",
      "                   cat          4          4      0.837          1      0.995      0.865\n",
      "                   dog          9          9      0.775      0.889      0.925       0.71\n",
      "                 horse          1          2      0.536          1      0.995      0.796\n",
      "              elephant          4         17      0.888      0.931      0.943      0.741\n",
      "                  bear          1          1      0.628          1      0.995      0.995\n",
      "                 zebra          2          4      0.853          1      0.995      0.971\n",
      "               giraffe          4          9      0.834      0.889      0.947      0.717\n",
      "              backpack          4          6      0.829      0.333      0.406       0.24\n",
      "              umbrella          4         18       0.96      0.556      0.753      0.468\n",
      "               handbag          9         19      0.347     0.0526      0.166       0.11\n",
      "                   tie          6          7      0.889      0.571      0.787      0.574\n",
      "              suitcase          2          4      0.815          1      0.995      0.659\n",
      "               frisbee          5          5      0.697        0.8       0.76      0.704\n",
      "                  skis          1          1      0.687          1      0.995      0.597\n",
      "             snowboard          2          7      0.813      0.628      0.821      0.489\n",
      "           sports ball          6          6       0.57       0.45      0.504      0.333\n",
      "                  kite          2         10      0.641        0.4      0.566      0.207\n",
      "          baseball bat          4          4          1          0      0.226     0.0863\n",
      "        baseball glove          4          7          1      0.423       0.43      0.226\n",
      "            skateboard          3          5      0.938        0.6      0.625      0.449\n",
      "         tennis racket          5          7      0.793      0.551      0.612      0.352\n",
      "                bottle          6         18      0.698      0.386      0.497      0.305\n",
      "            wine glass          5         16       0.78      0.443      0.723      0.376\n",
      "                   cup         10         36      0.685      0.306      0.406      0.281\n",
      "                  fork          6          6      0.595      0.167      0.264      0.216\n",
      "                 knife          7         16      0.697        0.5      0.622       0.38\n",
      "                 spoon          5         22      0.905      0.273      0.436      0.278\n",
      "                  bowl          9         28      0.624      0.714       0.68      0.557\n",
      "                banana          1          1      0.465          1      0.995      0.895\n",
      "              sandwich          2          2      0.479      0.479      0.663      0.663\n",
      "                orange          1          4          1          0      0.995      0.714\n",
      "              broccoli          4         11      0.477      0.182      0.239      0.199\n",
      "                carrot          3         24      0.683        0.5      0.682      0.468\n",
      "               hot dog          1          2      0.546          1      0.995      0.995\n",
      "                 pizza          5          5      0.674          1      0.995      0.844\n",
      "                 donut          2         14      0.586          1      0.924       0.82\n",
      "                  cake          4          4      0.639          1      0.995       0.82\n",
      "                 chair          9         35      0.521      0.543      0.545      0.322\n",
      "                 couch          5          6      0.561      0.667      0.713      0.549\n",
      "          potted plant          9         14      0.895      0.611      0.744      0.533\n",
      "                   bed          3          3      0.624      0.581      0.746      0.557\n",
      "          dining table         10         13      0.555      0.538       0.54      0.451\n",
      "                toilet          2          2      0.623        0.5      0.572      0.558\n",
      "                    tv          2          2       0.68          1      0.995      0.821\n",
      "                laptop          2          3          1      0.508      0.863      0.738\n",
      "                 mouse          2          2          1          0     0.0327     0.0131\n",
      "                remote          5          8      0.695        0.5      0.512      0.444\n",
      "            cell phone          5          8          1          0      0.188      0.103\n",
      "             microwave          3          3      0.512          1      0.995      0.897\n",
      "                  oven          5          5      0.342        0.4       0.42      0.337\n",
      "                  sink          4          6      0.313      0.167       0.27        0.2\n",
      "          refrigerator          5          5       0.86          1      0.995      0.628\n",
      "                  book          6         29       0.49      0.103      0.393      0.222\n",
      "                 clock          8          9      0.886      0.862      0.876       0.74\n",
      "                  vase          2          2      0.463          1      0.995      0.895\n",
      "              scissors          1          1          0          0      0.142     0.0142\n",
      "            teddy bear          6         21      0.759      0.381      0.711      0.493\n",
      "            toothbrush          2          5      0.978        0.8      0.962      0.633\n",
      "Speed: 5.5ms preprocess, 2.9ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1m/home/lukas-kurz/Desktop/autonomous_driving/intro_to_autonomous_vehicles_2025/task1/runs/detect/train7\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>‚ñÅ‚ñà</td></tr><tr><td>lr/pg1</td><td>‚ñÅ‚ñà</td></tr><tr><td>lr/pg2</td><td>‚ñÅ‚ñà</td></tr><tr><td>metrics/mAP50(B)</td><td>‚ñÅ‚ñà</td></tr><tr><td>metrics/mAP50-95(B)</td><td>‚ñÅ‚ñà</td></tr><tr><td>metrics/precision(B)</td><td>‚ñÅ‚ñà</td></tr><tr><td>metrics/recall(B)</td><td>‚ñà‚ñÅ</td></tr><tr><td>model/GFLOPs</td><td>‚ñÅ</td></tr><tr><td>model/parameters</td><td>‚ñÅ</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>‚ñÅ</td></tr><tr><td>train/box_loss</td><td>‚ñà‚ñÅ</td></tr><tr><td>train/cls_loss</td><td>‚ñà‚ñÅ</td></tr><tr><td>train/dfl_loss</td><td>‚ñà‚ñÅ</td></tr><tr><td>val/box_loss</td><td>‚ñà‚ñÅ</td></tr><tr><td>val/cls_loss</td><td>‚ñà‚ñÅ</td></tr><tr><td>val/dfl_loss</td><td>‚ñà‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>1e-05</td></tr><tr><td>lr/pg1</td><td>1e-05</td></tr><tr><td>lr/pg2</td><td>1e-05</td></tr><tr><td>metrics/mAP50(B)</td><td>0.68465</td></tr><tr><td>metrics/mAP50-95(B)</td><td>0.51857</td></tr><tr><td>metrics/precision(B)</td><td>0.70869</td></tr><tr><td>metrics/recall(B)</td><td>0.59221</td></tr><tr><td>model/GFLOPs</td><td>6.614</td></tr><tr><td>model/parameters</td><td>2624080</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>2.889</td></tr><tr><td>train/box_loss</td><td>1.20262</td></tr><tr><td>train/cls_loss</td><td>1.37885</td></tr><tr><td>train/dfl_loss</td><td>1.23246</td></tr><tr><td>val/box_loss</td><td>1.05543</td></tr><tr><td>val/cls_loss</td><td>0.99289</td></tr><tr><td>val/dfl_loss</td><td>1.1049</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">train7</strong> at: <a href='https://wandb.ai/kurzlukas-johannes-kepler-universit-t-linz/Ultralytics/runs/3db6vcad' target=\"_blank\">https://wandb.ai/kurzlukas-johannes-kepler-universit-t-linz/Ultralytics/runs/3db6vcad</a><br> View project at: <a href='https://wandb.ai/kurzlukas-johannes-kepler-universit-t-linz/Ultralytics' target=\"_blank\">https://wandb.ai/kurzlukas-johannes-kepler-universit-t-linz/Ultralytics</a><br>Synced 5 W&B file(s), 21 media file(s), 10 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250310_140109-3db6vcad/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a model\n",
    "model = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data=\"coco128.yaml\", epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33eda81",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112d983a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8696a834",
   "metadata": {},
   "source": [
    "TODO evaluate the model, consult this website.\n",
    "\n",
    "https://docs.ultralytics.com/modes/val/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c976c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "metrics = model.val()  # no arguments needed, dataset and settings remembered\n",
    "metrics.box.map  # map50-95\n",
    "metrics.box.map50  # map50\n",
    "metrics.box.map75  # map75\n",
    "metrics.box.maps  # a list contains map50-95 of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbea350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2811d5e4",
   "metadata": {},
   "source": [
    "## Video Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822959f0",
   "metadata": {},
   "source": [
    "For our task, we should create an annotate result from some video. With this, we should use a video of road traffic, which we then annotate with our trained model, image by image, comparing different trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66469e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11n.pt\")  # pretrained YOLO11n model\n",
    "\n",
    "# Run batched inference on a list of images\n",
    "results = model([\"image1.jpg\", \"image2.jpg\"])  # return a list of Results objects\n",
    "\n",
    "# Process results list\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "    result.show()  # display to screen\n",
    "    result.save(filename=\"result.jpg\")  # save to disk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
